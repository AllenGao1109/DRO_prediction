{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/47/91sqyf453hs814g86jzgnwdh0000gn/T/ipykernel_72986/3692189692.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: dict(zip(x['interview_date'], x['site_id_l'])))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "site_df = pd.read_csv('data/abcd_y_lt.csv').iloc[:,[0,2,7]]\n",
    "testdata = pd.read_csv('data/test.csv')\n",
    "testdata = testdata[testdata.columns[~testdata.columns.str.startswith(\"cbcl\")]]\n",
    "traindata = pd.read_csv('data/train.csv')\n",
    "traindata = traindata[traindata.columns[~traindata.columns.str.startswith(\"cbcl\")]]\n",
    "lostcolumns= [\"interview_date\",\"site\",\"src_subject_id\",\"y_t\", \"y_{t+1}\", \"eventname\",\"race_ethnicity\",\"urban\"]\n",
    "site_df['interview_date'] = pd.to_datetime(site_df['interview_date'], errors = 'coerce')\n",
    "traindata['interview_date'] = pd.to_datetime(traindata['interview_date'], errors = 'coerce')\n",
    "testdata['interview_date'] = pd.to_datetime(testdata['interview_date'], errors = 'coerce')\n",
    "site_df['site_id_l'] = site_df['site_id_l'].str[4:].astype(int)\n",
    "subject_date_site_mapping = (\n",
    "    site_df.groupby('src_subject_id')\n",
    "    .apply(lambda x: dict(zip(x['interview_date'], x['site_id_l'])))\n",
    "    .to_dict()\n",
    ")\n",
    "def get_site(row):\n",
    "    subject_id = row['src_subject_id']\n",
    "    interview_date = row['interview_date']\n",
    "    return subject_date_site_mapping.get(subject_id, {}).get(interview_date, None)\n",
    "\n",
    "df_urban = pd.read_csv(\"data/led_l_urban.csv\")\n",
    "traindata['site'] = traindata.apply(get_site, axis=1)\n",
    "traindata.insert(1, 'site', traindata.pop('site'))\n",
    "traindata = traindata.merge(\n",
    "    df_urban[['src_subject_id', 'reshist_addr1_urban_area']],\n",
    "    on='src_subject_id',\n",
    "    how='left'  \n",
    ")\n",
    "traindata.insert(1, 'urban', traindata.pop('reshist_addr1_urban_area'))\n",
    "\n",
    "\n",
    "testdata['site'] = traindata.apply(get_site, axis=1)\n",
    "testdata.insert(1, 'site', testdata.pop('site'))  \n",
    "testdata = testdata.merge(\n",
    "    df_urban[['src_subject_id', 'reshist_addr1_urban_area']],\n",
    "    on='src_subject_id',\n",
    "    how='left' \n",
    ")\n",
    "testdata.insert(1, 'urban', testdata.pop('reshist_addr1_urban_area'))  \n",
    "\n",
    "testdata = testdata.apply(pd.to_numeric,errors='coerce')\n",
    "\n",
    "traindata = traindata.apply(pd.to_numeric,errors='coerce')\n",
    "\n",
    "\n",
    "top_vars = [x for x in testdata.columns if x not in lostcolumns]\n",
    "\n",
    "\n",
    "testdata['white'] = (testdata['race_ethnicity'] == 1).astype(int)\n",
    "testdata['black'] = (testdata['race_ethnicity'] == 2).astype(int)\n",
    "testdata['hispano'] = (testdata['race_ethnicity'] == 3).astype(int)\n",
    "testdata['asian'] = (testdata['race_ethnicity'] == 4).astype(int)\n",
    "testdata['others'] = (testdata['race_ethnicity'] == 5).astype(int)\n",
    "\n",
    "\n",
    "traindata['white'] = (traindata['race_ethnicity'] == 1).astype(int)\n",
    "traindata['black'] = (traindata['race_ethnicity'] == 2).astype(int)\n",
    "traindata['hispano'] = (traindata['race_ethnicity'] == 3).astype(int)\n",
    "traindata['asian'] = (traindata['race_ethnicity'] == 4).astype(int)\n",
    "traindata['others'] = (traindata['race_ethnicity'] == 5).astype(int)\n",
    "\n",
    "data_train, hatdata = train_test_split(traindata, test_size=0.2, random_state=42)\n",
    "\n",
    "data_test =testdata\n",
    "\n",
    "X_train_df = data_train.drop(columns=lostcolumns)\n",
    "X_test_df = data_test.drop(columns=lostcolumns)\n",
    "feature_names = X_train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, hidden_dim=64, dropout_rate=0.3):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, input_dim, num_classes=4, learning_rate=0.01, num_epochs=6000):\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.model = None\n",
    "        self.best_model_params = None\n",
    "\n",
    "    def _prepare_data(self, X, y):\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y.values, dtype=torch.float32).view(-1, 1)\n",
    "        return X_tensor, y_tensor\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.model(torch.tensor(X_test, dtype=torch.float32))\n",
    "            y_prob = F.softmax(y_pred, dim=1).numpy()\n",
    "        auc_value = roc_auc_score((y_test == 3).astype(int), y_prob[:, 3])\n",
    "        return auc_value, y_prob[:, 3]\n",
    "\n",
    "class TraditionalMethod(ModelTrainer):\n",
    "    def __init__(self, input_dim, num_classes=4, learning_rate=0.01, num_epochs=6000):\n",
    "        super().__init__(input_dim, num_classes, learning_rate, num_epochs)\n",
    "        self.model = MultiLayerPerceptron(input_dim=input_dim, num_classes=num_classes)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        X_train_tensor, y_train_tensor = self._prepare_data(X_train, y_train)\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            logits = self.model(X_train_tensor)\n",
    "            loss = F.cross_entropy(logits, y_train_tensor.view(-1).long(), reduction='mean')\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        self.best_model_params = {name: param.clone() for name, param in self.model.named_parameters()}\n",
    "        self.model.load_state_dict(self.best_model_params)\n",
    "        return self\n",
    "\n",
    "class RegularizationMethod(ModelTrainer):\n",
    "    def __init__(self, input_dim, num_classes=4, learning_rate=0.01, num_epochs=6000):\n",
    "        super().__init__(input_dim, num_classes, learning_rate, num_epochs)\n",
    "        self.model = MultiLayerPerceptron(input_dim=input_dim, num_classes=num_classes)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.lambdas = [0.000001,0.000005,0.00001,0.00005,0.0001, 0.001, 0.005,0.01,0.05]\n",
    "        self.best_lambda = None\n",
    "\n",
    "    def train(self, X_train, y_train, hatdata, top_vars):\n",
    "        X_train_tensor, y_train_tensor = self._prepare_data(X_train, y_train)\n",
    "        best_auc = -float(\"inf\")\n",
    "        for lambda_val in self.lambdas:\n",
    "            model = MultiLayerPerceptron(input_dim=self.input_dim, num_classes=self.num_classes)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "            for epoch in range(self.num_epochs):\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(X_train_tensor)\n",
    "                loss = F.cross_entropy(logits, y_train_tensor.view(-1).long(), reduction='mean')\n",
    "                l2_reg = lambda_val * sum(p.pow(2).sum() for p in model.parameters())\n",
    "                loss += l2_reg\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                X_val = hatdata[top_vars].values\n",
    "                y_val = hatdata[\"y_{t+1}\"].values\n",
    "                y_pred = model(torch.tensor(X_val, dtype=torch.float32))\n",
    "                y_prob = F.softmax(y_pred, dim=1).numpy()\n",
    "                val_auc = roc_auc_score((y_val == 3).astype(int), y_prob[:, 3])\n",
    "                if val_auc > best_auc:\n",
    "                    best_auc = val_auc\n",
    "                    self.best_lambda = lambda_val\n",
    "                    self.best_model_params = {name: param.clone() for name, param in model.named_parameters()}\n",
    "                print(best_auc)\n",
    "        self.model.load_state_dict(self.best_model_params)\n",
    "        return self\n",
    "\n",
    "class DROMethod(ModelTrainer):\n",
    "    def __init__(self, input_dim, num_classes=4, learning_rate=0.01, num_epochs=6000):\n",
    "        super().__init__(input_dim, num_classes, learning_rate, num_epochs)\n",
    "        self.model = MultiLayerPerceptron(input_dim=input_dim, num_classes=num_classes)\n",
    "        self.kappa_iter = [1e-7,3e-7,1e-6,2e-6, 3e-6,5e-6,7e-6,1e-5,5e-5,1e-4,5e-4,1e-3]\n",
    "        self.kappacoef = [1,1.2]\n",
    "        self.best_kappa = None\n",
    "        self.was = [18]\n",
    "        self.best_kappacoef = None\n",
    "        self.best_lambda = None\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "\n",
    "\n",
    "    def train_single_run(self, X_train, y_train, hatdata, top_vars):\n",
    "        X_train_values = X_train\n",
    "        X_train_tensor = torch.tensor(X_train_values, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "        best_auc = -float(\"inf\")\n",
    "        for kcoefiter in self.kappacoef:\n",
    "            for kappa in self.kappa_iter:\n",
    "                for wasserstein in self.was:\n",
    "                    model = MultiLayerPerceptron(input_dim=self.input_dim, num_classes=self.num_classes)\n",
    "                    optimizer = optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "                    lambda_param_raw = torch.nn.Parameter(torch.tensor(1.0))\n",
    "                    optimizer_lambda = optim.Adam([lambda_param_raw], lr=self.learning_rate)\n",
    "                    for epoch in range(self.num_epochs):\n",
    "                        optimizer.zero_grad()\n",
    "                        optimizer_lambda.zero_grad()\n",
    "                        logits = model(X_train_tensor)\n",
    "                        loss = F.cross_entropy(logits, y_train_tensor.view(-1).long())\n",
    "                        l2_norm = torch.sqrt(sum(p.pow(2).sum() for p in model.parameters()))\n",
    "                        lambda_param = torch.exp(lambda_param_raw)\n",
    "                        y_onehot = torch.eye(logits.size(1))[y_train_tensor.view(-1).long()].to(X_train_tensor.device)\n",
    "                        true_logits = torch.sum(y_onehot * logits, dim=1)\n",
    "                        max_other_logits = torch.max((1 - y_onehot) * logits, dim=1)[0]\n",
    "                        margins = true_logits - max_other_logits - lambda_param * kappa\n",
    "                        label_uncertainty_term = torch.relu(margins).mean()\n",
    "                        loss += wasserstein * lambda_param\n",
    "                        loss += kcoefiter * label_uncertainty_term\n",
    "                        penalty = torch.relu(l2_norm - lambda_param).pow(2)\n",
    "                        loss += 500 * penalty\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        optimizer_lambda.step()\n",
    "                        with torch.no_grad():\n",
    "                            lambda_param_raw.data = torch.log(torch.max(torch.exp(lambda_param_raw), l2_norm + 1e-6))\n",
    "                            lambda_param_raw.data.clamp_(min=-10)\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        X_val = hatdata[top_vars].values\n",
    "                        y_val = hatdata[\"y_{t+1}\"].values\n",
    "                        y_pred = model(torch.tensor(X_val, dtype=torch.float32))\n",
    "                        y_prob = F.softmax(y_pred, dim=1).numpy()\n",
    "                        val_auc = roc_auc_score((y_val == 3).astype(int), y_prob[:, 3])\n",
    "                        if val_auc > best_auc:\n",
    "                            best_auc = val_auc\n",
    "                            self.best_kappa = kappa\n",
    "                            self.best_kappacoef = kcoefiter\n",
    "                            self.best_lambda = lambda_param.data\n",
    "                            self.best_model_params = {name: param.clone() for name, param in model.named_parameters()}\n",
    "                            self.best_was = wasserstein\n",
    "                        print(best_auc)\n",
    "        self.model.load_state_dict(self.best_model_params)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRA\n",
      "REG\n",
      "0.7925418847768705\n",
      "0.8001657172022137\n",
      "0.8005434292700826\n",
      "0.8007557732813466\n",
      "0.8007557732813466\n",
      "0.8007557732813466\n",
      "0.8007557732813466\n",
      "0.8007557732813466\n",
      "0.8007557732813466\n",
      "0.8007557732813466\n",
      "0.8007557732813466\n",
      "0.8007557732813466\n",
      "0.8007557732813466\n",
      "0.7941377638718242\n",
      "Best lambda (Reg): 5e-05\n",
      "0\n",
      "0.5990703839459041\n",
      "0.7926238546437001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "X_train = X_train_df[top_vars].values\n",
    "y_train = data_train[\"y_{t+1}\"]\n",
    "X_test = X_test_df[top_vars].values\n",
    "y_test = data_test[\"y_{t+1}\"].values\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "print(\"TRA\")\n",
    "trad_model = TraditionalMethod(input_dim=input_dim)\n",
    "trad_model.train(X_train, y_train)\n",
    "trad_auc, trad_probs = trad_model.evaluate(X_test, y_test)\n",
    "print(\"REG\")\n",
    "reg_model = RegularizationMethod(input_dim=input_dim)\n",
    "reg_model.train(X_train, y_train, hatdata, top_vars)\n",
    "reg_auc, reg_probs = reg_model.evaluate(X_test, y_test)\n",
    "print(reg_auc)\n",
    "print(\"Best lambda (Reg):\", reg_model.best_lambda)\n",
    "\n",
    "\n",
    "def train_dro_multiple_runs(n_runs, X_train, y_train, hatdata, top_vars, X_test, y_test):\n",
    "    probs_list = []\n",
    "    auc_list = []\n",
    "    best_params = []\n",
    "    models = []  \n",
    "\n",
    "    for run in range(n_runs):\n",
    "        print(run)\n",
    "        model = DROMethod(input_dim=X_train.shape[1])\n",
    "        model.train_single_run(X_train, y_train, hatdata, top_vars)\n",
    "        auc_val, probs = model.evaluate(X_test, y_test)\n",
    "        print(auc_val)\n",
    "\n",
    "        probs_list.append(probs)\n",
    "        auc_list.append(auc_val)\n",
    "        models.append(model)\n",
    "\n",
    "        print({\n",
    "            'kappa': model.best_kappa,\n",
    "            'kappacoef': model.best_kappacoef,\n",
    "            'lambda': model.best_lambda.item(),\n",
    "            'was': model.best_was\n",
    "        })\n",
    "\n",
    "        best_params.append({\n",
    "            'kappa': model.best_kappa,\n",
    "            'kappacoef': model.best_kappacoef,\n",
    "            'lambda': model.best_lambda.item(),\n",
    "            'was': model.best_was\n",
    "        })\n",
    "\n",
    "    avg_probs = np.mean(np.stack(probs_list), axis=0)\n",
    "    fpr, tpr, _ = roc_curve((y_test == 3).astype(int), avg_probs)\n",
    "    avg_auc = auc(fpr, tpr)\n",
    "\n",
    "    return models, avg_auc, avg_probs, best_params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dro_models, dro_auc, dro_probs, dro_best_params = train_dro_multiple_runs(\n",
    "    n_runs=2,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    hatdata=hatdata,\n",
    "    top_vars=top_vars,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "\n",
    "state_dicts = [m.best_model_params for m in dro_models]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_true = (y_test == 3).astype(int)\n",
    "\n",
    "fpr_dro, tpr_dro, _ = roc_curve(y_true, dro_probs)\n",
    "auc_dro = auc(fpr_dro, tpr_dro)\n",
    "\n",
    "trad_probs = trad_probs[:, 3]\n",
    "fpr_trad, tpr_trad, _ = roc_curve(y_true, trad_probs)\n",
    "auc_trad = auc(fpr_trad, tpr_trad)\n",
    "\n",
    "reg_probs = reg_probs[:, 3]\n",
    "fpr_reg, tpr_reg, _ = roc_curve(y_true, reg_probs)\n",
    "auc_reg = auc(fpr_reg, tpr_reg)\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(fpr_dro, tpr_dro, label=f\"DRO (AUC = {auc_dro:.3f})\", linewidth=2)\n",
    "plt.plot(fpr_trad, tpr_trad, label=f\"Traditional (AUC = {auc_trad:.3f})\", linewidth=2)\n",
    "plt.plot(fpr_reg, tpr_reg, label=f\"Regularized (AUC = {auc_reg:.3f})\", linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve Comparison (class 3)\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trad_4_p = trad_probs\n",
    "dro_4_p = dro_probs\n",
    "reg_4_p = reg_probs\n",
    "trad_4_p = trad_4_p.tolist()\n",
    "reg_4_p = reg_4_p.tolist()\n",
    "dro_4_p = dro_4_p.tolist()\n",
    "indices_to_remove = testdata.index[testdata['y_t'] == 3].tolist()\n",
    "testdata_4 = testdata\n",
    "y_true_4 = y_true.tolist()\n",
    "\n",
    "for idx in sorted(indices_to_remove, reverse=True):\n",
    "    del trad_4_p[idx]\n",
    "    del reg_4_p[idx]\n",
    "    del dro_4_p[idx]\n",
    "    del y_true_4[idx]\n",
    "\n",
    "fpr_dro, tpr_dro, _ = roc_curve(y_true_4, dro_4_p)\n",
    "auc_dro = auc(fpr_dro, tpr_dro)\n",
    "\n",
    "fpr_trad, tpr_trad, _ = roc_curve(y_true_4, trad_4_p)\n",
    "auc_trad = auc(fpr_trad, tpr_trad)\n",
    "\n",
    "fpr_reg, tpr_reg, _ = roc_curve(y_true_4, reg_4_p)\n",
    "auc_reg = auc(fpr_reg, tpr_reg)\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(fpr_dro, tpr_dro, label=f\"DRO (AUC = {auc_dro:.3f})\", linewidth=2)\n",
    "plt.plot(fpr_trad, tpr_trad, label=f\"Traditional (AUC = {auc_trad:.3f})\", linewidth=2)\n",
    "plt.plot(fpr_reg, tpr_reg, label=f\"Regularized (AUC = {auc_reg:.3f})\", linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve Comparison (class 3)\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, hidden_dim=64, dropout_rate=0.3):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, input_dim, num_classes=4, learning_rate=0.01, num_epochs=2000):\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.model = None\n",
    "        self.best_model_params = None\n",
    "\n",
    "    def _prepare_data(self, X, y):\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y.values, dtype=torch.float32).view(-1, 1)\n",
    "        return X_tensor, y_tensor\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.model(torch.tensor(X_test, dtype=torch.float32))\n",
    "            y_prob = F.softmax(y_pred, dim=1).numpy()\n",
    "        auc_value = roc_auc_score((y_test == 3).astype(int), y_prob[:, 3])\n",
    "        return auc_value, y_prob[:, 3]\n",
    "\n",
    "\n",
    "class TraditionalMethod(ModelTrainer):\n",
    "    def __init__(self, input_dim, num_classes=4, learning_rate=0.01, num_epochs=4000):\n",
    "        super().__init__(input_dim, num_classes, learning_rate, num_epochs)\n",
    "        self.model = MultiLayerPerceptron(input_dim=input_dim, num_classes=num_classes)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        X_train_tensor, y_train_tensor = self._prepare_data(X_train, y_train)\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            logits = self.model(X_train_tensor)\n",
    "            loss = F.cross_entropy(\n",
    "                logits, y_train_tensor.view(-1).long(), reduction=\"mean\"\n",
    "            )\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        self.best_model_params = {\n",
    "            name: param.clone() for name, param in self.model.named_parameters()\n",
    "        }\n",
    "        self.model.load_state_dict(self.best_model_params)\n",
    "        return self\n",
    "\n",
    "\n",
    "class RegularizationMethod(ModelTrainer):\n",
    "    def __init__(self, input_dim, num_classes=4, learning_rate=0.01, num_epochs=4000):\n",
    "        super().__init__(input_dim, num_classes, learning_rate, num_epochs)\n",
    "        self.model = MultiLayerPerceptron(input_dim=input_dim, num_classes=num_classes)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.lambdas = [\n",
    "            0.0001,\n",
    "            0.001,\n",
    "            0.005,\n",
    "            0.01,\n",
    "            0.05,\n",
    "            0.1,\n",
    "            0.2,\n",
    "            0.3,\n",
    "            0.4,\n",
    "        ]\n",
    "        self.best_lambda = None\n",
    "\n",
    "    def train(self, X_train, y_train, hatdata, top_vars):\n",
    "        X_train_tensor, y_train_tensor = self._prepare_data(X_train, y_train)\n",
    "        best_auc = -float(\"inf\")\n",
    "        for lambda_val in self.lambdas:\n",
    "            model = MultiLayerPerceptron(\n",
    "                input_dim=self.input_dim, num_classes=self.num_classes\n",
    "            )\n",
    "            optimizer = optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "            for epoch in range(self.num_epochs):\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(X_train_tensor)\n",
    "                loss = F.cross_entropy(\n",
    "                    logits, y_train_tensor.view(-1).long(), reduction=\"mean\"\n",
    "                )\n",
    "                l2_reg = lambda_val * sum(p.pow(2).sum() for p in model.parameters())\n",
    "                loss += l2_reg\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                X_val = hatdata[top_vars].values\n",
    "                y_val = hatdata[\"y_{t+1}\"].values\n",
    "                y_pred = model(torch.tensor(X_val, dtype=torch.float32))\n",
    "                y_prob = F.softmax(y_pred, dim=1).numpy()\n",
    "                val_auc = roc_auc_score((y_val == 3).astype(int), y_prob[:, 3])\n",
    "                if val_auc > best_auc:\n",
    "                    best_auc = val_auc\n",
    "                    self.best_lambda = lambda_val\n",
    "                    self.best_model_params = {\n",
    "                        name: param.clone() for name, param in model.named_parameters()\n",
    "                    }\n",
    "                print(best_auc)\n",
    "        self.model.load_state_dict(self.best_model_params)\n",
    "        return self\n",
    "\n",
    "\n",
    "class DROMethod(ModelTrainer):\n",
    "    def __init__(self, input_dim, num_classes=4, learning_rate=0.01, num_epochs=6000):\n",
    "        super().__init__(input_dim, num_classes, learning_rate, num_epochs)\n",
    "        self.model = MultiLayerPerceptron(input_dim=input_dim, num_classes=num_classes)\n",
    "        self.kappa_iter = [\n",
    "            2e-6,\n",
    "            3e-6,\n",
    "            5e-6,\n",
    "            7e-6,\n",
    "            1e-5,\n",
    "            5e-5,\n",
    "            1e-4,\n",
    "            5e-4,\n",
    "            1e-3,\n",
    "        ]\n",
    "        self.kappacoef = [1, 1.2]\n",
    "        self.best_kappa = None\n",
    "        self.was = [18]\n",
    "        self.best_kappacoef = None\n",
    "        self.best_lambda = None\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def train_single_run(self, X_train, y_train, hatdata, top_vars):\n",
    "        X_train_values = X_train\n",
    "        X_train_tensor = torch.tensor(X_train_values, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "        best_auc = -float(\"inf\")\n",
    "        for kcoefiter in self.kappacoef:\n",
    "            for kappa in self.kappa_iter:\n",
    "                for wasserstein in self.was:\n",
    "                    model = MultiLayerPerceptron(\n",
    "                        input_dim=self.input_dim, num_classes=self.num_classes\n",
    "                    )\n",
    "                    optimizer = optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "                    lambda_param_raw = torch.nn.Parameter(torch.tensor(1.0))\n",
    "                    optimizer_lambda = optim.Adam(\n",
    "                        [lambda_param_raw], lr=self.learning_rate\n",
    "                    )\n",
    "                    for epoch in range(self.num_epochs):\n",
    "                        optimizer.zero_grad()\n",
    "                        optimizer_lambda.zero_grad()\n",
    "                        logits = model(X_train_tensor)\n",
    "                        loss = F.cross_entropy(logits, y_train_tensor.view(-1).long())\n",
    "                        l2_norm = torch.sqrt(\n",
    "                            sum(p.pow(2).sum() for p in model.parameters())\n",
    "                        )\n",
    "                        lambda_param = torch.exp(lambda_param_raw)\n",
    "                        y_onehot = torch.eye(logits.size(1))[\n",
    "                            y_train_tensor.view(-1).long()\n",
    "                        ].to(X_train_tensor.device)\n",
    "                        true_logits = torch.sum(y_onehot * logits, dim=1)\n",
    "                        max_other_logits = torch.max((1 - y_onehot) * logits, dim=1)[0]\n",
    "                        margins = true_logits - max_other_logits - lambda_param * kappa\n",
    "                        label_uncertainty_term = torch.relu(margins).mean()\n",
    "                        loss += wasserstein * lambda_param\n",
    "                        loss += kcoefiter * label_uncertainty_term\n",
    "                        penalty = torch.relu(l2_norm - lambda_param).pow(2)\n",
    "                        loss += 500 * penalty\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        optimizer_lambda.step()\n",
    "                        with torch.no_grad():\n",
    "                            lambda_param_raw.data = torch.log(\n",
    "                                torch.max(torch.exp(lambda_param_raw), l2_norm + 1e-6)\n",
    "                            )\n",
    "                            lambda_param_raw.data.clamp_(min=-10)\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        X_val = hatdata[top_vars].values\n",
    "                        y_val = hatdata[\"y_{t+1}\"].values\n",
    "                        y_pred = model(torch.tensor(X_val, dtype=torch.float32))\n",
    "                        y_prob = F.softmax(y_pred, dim=1).numpy()\n",
    "                        val_auc = roc_auc_score((y_val == 3).astype(int), y_prob[:, 3])\n",
    "                        if val_auc > best_auc:\n",
    "                            best_auc = val_auc\n",
    "                            self.best_kappa = kappa\n",
    "                            self.best_kappacoef = kcoefiter\n",
    "                            self.best_lambda = lambda_param.data\n",
    "                            self.best_model_params = {\n",
    "                                name: param.clone()\n",
    "                                for name, param in model.named_parameters()\n",
    "                            }\n",
    "                            self.best_was = wasserstein\n",
    "                    print(best_auc)\n",
    "        self.model.load_state_dict(self.best_model_params)\n",
    "        return self\n",
    "\n",
    "\n",
    "def train_and_eval_traditional(X_train, y_train, data, top_vars, filtered_list):\n",
    "    \"\"\"训练并评估传统方法\"\"\"\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = TraditionalMethod(input_dim=input_dim)\n",
    "    model.train(X_train, y_train)\n",
    "\n",
    "    auc_list, pro_list = [], []\n",
    "    auc_list_4, pro_list_4 = [], []\n",
    "\n",
    "    for site in filtered_list:\n",
    "        df_site = data[data[\"site\"] == site]\n",
    "        X_site = df_site[top_vars].values\n",
    "        y_site = df_site[\"y_{t+1}\"].values\n",
    "        auc, proba = model.evaluate(X_site, y_site)\n",
    "        auc_list.append(auc)\n",
    "        pro_list.append(proba.reshape(-1, 1))\n",
    "\n",
    "        df_site_4 = df_site[df_site[\"y_t\"] != 3]\n",
    "        X_site_4 = df_site_4[top_vars].values\n",
    "        y_site_4 = df_site_4[\"y_{t+1}\"].values\n",
    "        auc_4, proba_4 = model.evaluate(X_site_4, y_site_4)\n",
    "        auc_list_4.append(auc_4)\n",
    "        pro_list_4.append(proba_4.reshape(-1, 1))\n",
    "\n",
    "    return model, auc_list, pro_list, auc_list_4, pro_list_4\n",
    "\n",
    "\n",
    "def train_and_eval_regularization(\n",
    "    X_train, y_train, hatdata, top_vars, data, filtered_list\n",
    "):\n",
    "    \"\"\"训练并评估正则化方法\"\"\"\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = RegularizationMethod(input_dim=input_dim)\n",
    "    model.train(X_train, y_train, hatdata, top_vars)\n",
    "\n",
    "    auc_list, pro_list = [], []\n",
    "    auc_list_4, pro_list_4 = [], []\n",
    "    white_list, white_list_4 = [], []\n",
    "\n",
    "    for site in filtered_list:\n",
    "        df_site = data[data[\"site\"] == site]\n",
    "        X_site = df_site[top_vars].values\n",
    "        y_site = df_site[\"y_{t+1}\"].values\n",
    "        auc, proba = model.evaluate(X_site, y_site)\n",
    "        auc_list.append(auc)\n",
    "        pro_list.append(proba.reshape(-1, 1))\n",
    "\n",
    "        df_site_4 = df_site[df_site[\"y_t\"] != 3]\n",
    "        X_site_4 = df_site_4[top_vars].values\n",
    "        y_site_4 = df_site_4[\"y_{t+1}\"].values\n",
    "        auc_4, proba_4 = model.evaluate(X_site_4, y_site_4)\n",
    "        auc_list_4.append(auc_4)\n",
    "        pro_list_4.append(proba_4.reshape(-1, 1))\n",
    "\n",
    "        df_white = df_site[df_site[\"race_ethnicity\"] != 1]\n",
    "        X_white = df_white[top_vars].values\n",
    "        y_white = df_white[\"y_{t+1}\"].values\n",
    "        auc_white, _ = model.evaluate(X_white, y_white)\n",
    "        white_list.append(auc_white)\n",
    "\n",
    "        df_white_4 = df_white[df_white[\"y_t\"] != 3]\n",
    "        X_white_4 = df_white_4[top_vars].values\n",
    "        y_white_4 = df_white_4[\"y_{t+1}\"].values\n",
    "        auc_white_4, _ = model.evaluate(X_white_4, y_white_4)\n",
    "        white_list_4.append(auc_white_4)\n",
    "\n",
    "    return model, auc_list, pro_list, auc_list_4, pro_list_4, white_list, white_list_4\n",
    "\n",
    "\n",
    "def train_and_eval_dro_multiple(\n",
    "    n_runs, X_train, y_train, hatdata, top_vars, data, filtered_list\n",
    "):\n",
    "    \"\"\"多次训练并评估DRO方法，返回平均结果\"\"\"\n",
    "    input_dim = X_train.shape[1]\n",
    "    auc_list = None\n",
    "    pro_list = None\n",
    "    auc_list_4 = None\n",
    "    pro_list_4 = None\n",
    "    white_list = None\n",
    "    white_list_4 = None\n",
    "    best_params = []\n",
    "    models = []\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        print(run)\n",
    "        model = DROMethod(input_dim=input_dim)\n",
    "        model.train_single_run(X_train, y_train, hatdata, top_vars)\n",
    "        models.append(model)\n",
    "\n",
    "        run_auc, run_pro, run_auc_4, run_pro_4, run_white, run_white_4 = (\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "        )\n",
    "        for site in filtered_list:\n",
    "            df_site = data[data[\"site\"] == site]\n",
    "            X_site = df_site[top_vars].values\n",
    "            y_site = df_site[\"y_{t+1}\"].values\n",
    "            auc, proba = model.evaluate(X_site, y_site)\n",
    "            run_auc.append(auc)\n",
    "            run_pro.append(proba.reshape(-1, 1))\n",
    "\n",
    "            df_site_4 = df_site[df_site[\"y_t\"] != 3]\n",
    "            X_site_4 = df_site_4[top_vars].values\n",
    "            y_site_4 = df_site_4[\"y_{t+1}\"].values\n",
    "            auc_4, proba_4 = model.evaluate(X_site_4, y_site_4)\n",
    "            run_auc_4.append(auc_4)\n",
    "            run_pro_4.append(proba_4.reshape(-1, 1))\n",
    "\n",
    "            df_white = df_site[df_site[\"race_ethnicity\"] != 1]\n",
    "            X_white = df_white[top_vars].values\n",
    "            y_white = df_white[\"y_{t+1}\"].values\n",
    "            auc_white, _ = model.evaluate(X_white, y_white)\n",
    "            run_white.append(auc_white)\n",
    "\n",
    "            df_white_4 = df_white[df_white[\"y_t\"] != 3]\n",
    "            X_white_4 = df_white_4[top_vars].values\n",
    "            y_white_4 = df_white_4[\"y_{t+1}\"].values\n",
    "            auc_white_4, _ = model.evaluate(X_white_4, y_white_4)\n",
    "            run_white_4.append(auc_white_4)\n",
    "\n",
    "        # 累加结果\n",
    "        if auc_list is None:\n",
    "            auc_list = run_auc\n",
    "            pro_list = run_pro\n",
    "            auc_list_4 = run_auc_4\n",
    "            pro_list_4 = run_pro_4\n",
    "            white_list = run_white\n",
    "            white_list_4 = run_white_4\n",
    "        else:\n",
    "            auc_list = [a + b for a, b in zip(auc_list, run_auc)]\n",
    "            pro_list = [a + b for a, b in zip(pro_list, run_pro)]\n",
    "            auc_list_4 = [a + b for a, b in zip(auc_list_4, run_auc_4)]\n",
    "            pro_list_4 = [a + b for a, b in zip(pro_list_4, run_pro_4)]\n",
    "            white_list = [a + b for a, b in zip(white_list, run_white)]\n",
    "            white_list_4 = [a + b for a, b in zip(white_list_4, run_white_4)]\n",
    "\n",
    "        best_params.append(\n",
    "            {\n",
    "                \"kappa\": model.best_kappa,\n",
    "                \"kappacoef\": model.best_kappacoef,\n",
    "                \"lambda\": float(model.best_lambda)\n",
    "                if hasattr(model.best_lambda, \"item\")\n",
    "                else model.best_lambda,\n",
    "                \"was\": model.best_was,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # 求平均\n",
    "    auc_list = [x / n_runs for x in auc_list]\n",
    "    pro_list = [x / n_runs for x in pro_list]\n",
    "    auc_list_4 = [x / n_runs for x in auc_list_4]\n",
    "    pro_list_4 = [x / n_runs for x in pro_list_4]\n",
    "    white_list = [x / n_runs for x in white_list]\n",
    "    white_list_4 = [x / n_runs for x in white_list_4]\n",
    "\n",
    "    return (\n",
    "        models,\n",
    "        auc_list,\n",
    "        pro_list,\n",
    "        auc_list_4,\n",
    "        pro_list_4,\n",
    "        white_list,\n",
    "        white_list_4,\n",
    "        best_params,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def tune_traditional_with_optuna(X_train_all, y_train_all, hatdata, top_vars, n_trials=30):\n",
    "    \"\"\"使用 Optuna 调参传统方法并返回最优模型和参数\"\"\"\n",
    "\n",
    "    def objective(trial):\n",
    "        # 超参数空间\n",
    "        hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 256, step=32)\n",
    "        dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "\n",
    "        # 拆分为训练 + 验证\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_all, y_train_all, test_size=0.2, stratify=y_train_all, random_state=42\n",
    "        )\n",
    "\n",
    "        # 创建模型\n",
    "        model = MultiLayerPerceptron(\n",
    "            input_dim=X_train.shape[1],\n",
    "            num_classes=4,\n",
    "            hidden_dim=hidden_dim,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "        trainer = TraditionalMethod(\n",
    "            input_dim=X_train.shape[1],\n",
    "            learning_rate=learning_rate,\n",
    "            num_epochs=300  # 较小的epoch用于调参\n",
    "        )\n",
    "        trainer.model = model\n",
    "        trainer.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        trainer.train(X_train, y_train)\n",
    "\n",
    "        auc, _ = trainer.evaluate(X_val, y_val)\n",
    "        return auc\n",
    "\n",
    "    # Optuna 搜索\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # 用最佳参数重新训练（全epoch）\n",
    "    best_params = study.best_params\n",
    "    print(\"Best AUC:\", study.best_value)\n",
    "    print(\"Best params:\", best_params)\n",
    "\n",
    "    final_model = TraditionalMethod(\n",
    "        input_dim=X_train_all.shape[1],\n",
    "        learning_rate=best_params[\"learning_rate\"],\n",
    "        num_epochs=4000\n",
    "    )\n",
    "    final_model.model = MultiLayerPerceptron(\n",
    "        input_dim=X_train_all.shape[1],\n",
    "        num_classes=4,\n",
    "        hidden_dim=best_params[\"hidden_dim\"],\n",
    "        dropout_rate=best_params[\"dropout_rate\"]\n",
    "    )\n",
    "    final_model.optimizer = optim.Adam(\n",
    "        final_model.model.parameters(), lr=best_params[\"learning_rate\"]\n",
    "    )\n",
    "    final_model.train(X_train_all, y_train_all)\n",
    "\n",
    "    return final_model, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 20:28:31,553] A new study created in memory with name: no-name-be23f1b0-ecec-4cce-841e-0284942668a7\n",
      "[W 2025-04-23 20:28:32,138] Trial 0 failed with parameters: {'hidden_dim': 128, 'dropout_rate': 0.48630785620850503, 'learning_rate': 0.00016356825485146068} because of the following error: ValueError(\"could not determine the shape of object type 'DataFrame'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gaozhiyuan/.pyenv/versions/3.12.1/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/folders/47/91sqyf453hs814g86jzgnwdh0000gn/T/ipykernel_65217/3764960907.py\", line 419, in objective\n",
      "    trainer.train(X_train, y_train)\n",
      "  File \"/var/folders/47/91sqyf453hs814g86jzgnwdh0000gn/T/ipykernel_65217/3764960907.py\", line 56, in train\n",
      "    X_train_tensor, y_train_tensor = self._prepare_data(X_train, y_train)\n",
      "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/47/91sqyf453hs814g86jzgnwdh0000gn/T/ipykernel_65217/3764960907.py\", line 36, in _prepare_data\n",
      "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: could not determine the shape of object type 'DataFrame'\n",
      "[W 2025-04-23 20:28:32,139] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not determine the shape of object type 'DataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m final_model, best_params = \u001b[43mtune_traditional_with_optuna\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_all\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_train_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train_all\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43my_\u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43mt+1}\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhatdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhatdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 可以改成你想要的搜索轮数\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 426\u001b[39m, in \u001b[36mtune_traditional_with_optuna\u001b[39m\u001b[34m(X_train_all, y_train_all, hatdata, top_vars, n_trials)\u001b[39m\n\u001b[32m    424\u001b[39m \u001b[38;5;66;03m# Optuna 搜索\u001b[39;00m\n\u001b[32m    425\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[38;5;66;03m# 用最佳参数重新训练（全epoch）\u001b[39;00m\n\u001b[32m    429\u001b[39m best_params = study.best_params\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.1/lib/python3.12/site-packages/optuna/study/study.py:475\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    375\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.1/lib/python3.12/site-packages/optuna/study/_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.1/lib/python3.12/site-packages/optuna/study/_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.1/lib/python3.12/site-packages/optuna/study/_optimize.py:248\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    244\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    245\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    247\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.1/lib/python3.12/site-packages/optuna/study/_optimize.py:197\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    199\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    200\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 419\u001b[39m, in \u001b[36mtune_traditional_with_optuna.<locals>.objective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    417\u001b[39m trainer.model = model\n\u001b[32m    418\u001b[39m trainer.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m auc, _ = trainer.evaluate(X_val, y_val)\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m auc\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mTraditionalMethod.train\u001b[39m\u001b[34m(self, X_train, y_train)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_train, y_train):\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     X_train_tensor, y_train_tensor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_epochs):\n\u001b[32m     58\u001b[39m         \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mModelTrainer._prepare_data\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prepare_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     X_tensor = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     y_tensor = torch.tensor(y.values, dtype=torch.float32).view(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m X_tensor, y_tensor\n",
      "\u001b[31mValueError\u001b[39m: could not determine the shape of object type 'DataFrame'"
     ]
    }
   ],
   "source": [
    "final_model, best_params = tune_traditional_with_optuna(\n",
    "    X_train_all=X_train_df,\n",
    "    y_train_all=data_train[\"y_{t+1}\"],\n",
    "    hatdata=hatdata,\n",
    "    top_vars=top_vars,\n",
    "    n_trials=30  # 可以改成你想要的搜索轮数\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
